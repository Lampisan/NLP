% (c) 2005 Lukasz Grzegorz Maciak
\documentclass[a4paper,10pt]{article}
\setlength{\textheight}{10in}\setlength{\textwidth}{6.5in}\setlength{\topmargin}{-0.125in}\setlength{\oddsidemargin}{-.2in}\setlength{\evensidemargin}{-.2in}\setlength{\headsep}{0.2in}\setlength{\footskip}{0pt}
\usepackage{amsmath}\usepackage{fancyhdr}\usepackage{enumitem}\usepackage{hyperref}\usepackage{graphicx}\usepackage{subcaption}\usepackage{libertine}\usepackage{bm}\usepackage{amssymb} \usepackage{tikz}\usepackage{fontawesome}\usepackage{todonotes}
\pagestyle{fancy}\lhead{Name: Alexander Palmisano}\chead{M.Number: 11707789}\rhead{NLP (706.230)}\fancyfoot{}
\begin{document}\begin{enumerate}[topsep=0mm, partopsep=0mm, leftmargin=*]


% Hint: change the \item to \item[\faCheckSquare] to make this your selection!

\item \textit{Text Representation}. What is the document-term matrix?
\begin{itemize}[topsep=0mm, partopsep=0mm, noitemsep, leftmargin=*, label=\faSquareO]
	\item Each term of a language is getting documented by specially trained linguists \\
		  \{ just does not make sense, also would be an unfeasible amount of work \}
	\item The latent representation of attention heads in Transformer blocks \\
		  \{ maps a query and a set of key-value pairs to an output \}
	\item[\faCheckSquare] Feature space spawned by terms over documents represented as a matrix
	\item Adjacency matrix of the bipartite graph of documents and the terms contained in each document \\
  	 	  \{ Adjacency matrix would be square, document-term matrix \#documents $\times$ \#terms, but from a logical standpoint the document term matrix is contained in the adjacency matrix\}
\end{itemize}

\item \textit{Assumptions}. Which are reasonable approaches to capture information in text?
\begin{itemize}[topsep=0mm, partopsep=0mm, noitemsep, leftmargin=*, label=\faSquareO]
	\item[\faCheckSquare] Words are influenced by their surrounding words in context
	\item The sentiment influences the word order in conjunctions \\
		  \{ we would expect no influence here, but could be tested \}
	\item[\faCheckSquare] Average sentence length depends on the author's specific writing style \\
		 \{ also does depend on other factors, e.g. what type of text is the author writing\}
	\item Lexems depends on the anaphoric resolution \\
		 \{ anaphoric resolution ... the problem of resolving what a pronoun, or a noun phrase refers to \\
		    lexeme ... root form of a word \\
			pronouns refer to a noun (e.g. I, you, this, what) and are probably learned as different lexems \\
			don't really see the connection between the two terms here\}
\end{itemize}

\item \textit{Learning}. Which of these statements is true?
\begin{itemize}[topsep=0mm, partopsep=0mm, noitemsep, leftmargin=*, label=\faSquareO]
	\item Deep learning is always preferred over rule-based approaches \\
		  \{ rule-based might be simpler, if it also solves the problem why not use it,  also rule-based approaches need far less data than deep learning\} 
	\item Rule-based approaches require large amounts of labelled training data \\
		  \{ rules are often created manually and then just applied, no need for large amount of data\}
	\item The industry often prefers rule-based approaches over machine learning \\
		  \{ can't really speak for the whole industry, but I guess that both approaches have valid applications, \\ some companies might also like the buzzwords for machine learning\}
	\item[\faCheckSquare] Deep learning is considered the state of the art for many NLP tasks \\
		  \{ deep learning currently produces the best results for almost all NLP tasks\}
\end{itemize}


\newpage \item \textit{Named Entity Recognition}. There are multiple ways on how to encode the tokens used to train sequence classification systems, like named entity recognition systems. 
\begin{enumerate}
	\item Why is a simple binary scheme like EO (entity token, outside token) not a good idea?
	\item What could be the reason that a BERT-based sequence classifier fail to learn \texttt{[O B I O $\cdots$]} and outputs \texttt{[O I I O $\cdots$]} instead?
\end{enumerate}

\begin{enumerate}
\item
	By only using entity and outside tokens we can't represent two entities next to each other. We have no tag to create a boundary between these two entities.
\item
	BERT does not learn the beginning token since it just learns what is a named entity and what is not a named entity. BERT might also have trouble with neighboring entities, does not really know when one entity ends and another begins. For example two first names are probably two different people or they could be the name of one person.
\end{enumerate}


\newpage \item \textit{Word Embeddings}. Traditional word embedding techniques like word2vec have been popular in past years, but today are less used and contextual embedding techniques are preferred. 
\begin{enumerate}
	\item What are the advantages of contextual word embeddings, like BERT, over classical word embeddings?
	\item Are there cases, where a traditional word embedding method is preferred? If yes, please provide an example.
\end{enumerate}

\begin{enumerate}
\item
	For classical word embeddings we would use the same embedding for words even if they are used in different contexts. For example ``computer mouse'' and ``mouse rodent'', in this case we would get the same word embedding vector for both (`global' word embedding). With contextual word embedding we would in the best case get two different embeddings depending on the context the word is used in.
\item
	Can't really think of any. In some cases the contextual word embedding would not provide any advantages, but I don't really see a case were it would actively hurt to use contextual word embeddings. One disadvantage of the contextual word embedding, like BERT, is that we need to know the model in order to use the embeddings, since the model generates the embedding based on the context. For traditional word embedding techniques we get a context independent vector that we can directly use.
\end{enumerate}


\newpage \item \textit{Deep Learning}. Many attention-based approaches combine as inputs a token embedding and a position embedding. 
\begin{enumerate}
	\item Why is there a position embedding, what is its purpose?
	\item Are there tasks, where the position embedding is not useful? If yes, please provide an example.
\end{enumerate}
\begin{enumerate}
\item
	The many attention-based approach does not really process the sequence of input tokens one by one (more of a parallel computation, sequence at once). Therefore the order of tokens gets lost. We believe that language has an order therefore we would like to keep this information intact. Otherwise the sentences ``cook kneads dough'' and ``dough kneads cook'' would be the same. This is achieved by additionally adding a position embedding, which could be either learned or fixed. Now the transformer is able to know the relative position of the words.
\item
	Text generation\\
	If we would like to generate text the transformer does not really need a position embedding for the input. Also if we only provide a single word as input the position does not matter.\\
	Speech to text\\
	Input is a recording or live feed of spoken text. The position is embedded in the temporal domain.\\
	Spam detection\\
	The model probably tries to identify key words that indicate a spam message and does not really care about the order of the text.\\
	Also everytime the input text consists out of words where the position of these words does not matter.
\end{enumerate}


\newpage \item \textit{Word Senses}. You are asked to develop a method for disambiguation of named entities, e.g., names of celebrities, based on Tweets. For example this tweet refers to Adam Scott, the golfer and not Adam Scott, the actor.
\begin{quote}\color{blue}
AmerExperience.com/Golf @ShopAmerGolf May 23 \\
~\\
FREE GOLF MAGAZINE
Golf: Will Zalatoris, Adam Scott, Keegan Bradley among those now exempt into 2022 U.S. Open | Flipboard $<$url$>$ Read for free 
\#golfnews \#golflessons \#golftraining
\end{quote}

\begin{enumerate}
	\item Are there any Twitter-specific features, which can be used for this task?
	\item Would there be other data sources that can be useful for the task?
	\item Briefly describe how you would solve the task. 
\end{enumerate}

\begin{enumerate}
\item
	\# hashtags, twitter id / @handle \\
	finding the celebrity accounts via verified accounts (no impersonators)
\item
	We could crawl or find a dataset that has the name of celebrities and some information about their field of work (sport, movie, politician, \dots) e.g. actor/actress imdb and use that to identify the topics of the tweet and to differentiate between two people that have the same name. Also some kind of graph that connects celebrities with each other would be helpful, for example cluster of sport teams, cluster of actors/actresses that play together in a movie, singers that colaborate, \dots\,. Could also make use of other social media networks.
\item
	We would crawl a dataset from twitter by using the twitter handle of celebrities. Now we know that these tweets correspond to the individual celebrities. We could train a model (probably some Transformer, even better if pre-trained) by masking out the mention of the twitter handle of the person and ask the model to predict which celebrity the tweet corresponds too. We know the ground truth and after training we could apply the model on tweets that to not contain the twitter handle. If only the name is given in these tweets without the handle we could also only look at celebrities with this specific name and query the model which of those persons is more likely that the tweet corresponds to. 
\end{enumerate}


\newpage \item \textit{Plagiarism}. Consider the university asks you to develop a system to test thesis (e.g., Bachelor and Master) for cases of plagiarism. Your system should for each thesis check, if there are plagiarised passages and mark the beginning and the end of a suspicious passage, which is then checked by human experts. 
\begin{enumerate}
	\item Which data sources would you consider? 
	\item What features would you use? (short list with explanation)
	\item What method would you choose?
	\item How well do you expect your method to work? What are the bottlenecks?
\end{enumerate}

\begin{enumerate}
\item  
	Consider larger knowledge bases in the web (wikipedia, ...), other bachelor thesis, master thesis and publications. If we know the field we could maybe narrow down the data needed. Also the submission of other students from the same course or previous years.  Previously known cases of plagiarism (probably hand labeled) will also be useful.
\item 
	 A simple feature to use would be n-grams and then compare the overlap. A more advanced idea would be to use POS tagging for semantic similarity. Or we could use some vector embedding like GloVe and then compute similarity features and containment. We should also decide if we do this on a word or sentence level. Also stylometry could be considered. If some part of the text has a vastly different style and is not cited it could be an indication for plagiarism.
\item 
	Some deep learning model. There are Deep Structured Sematic Models which might be a good fit for this task. Otherwise maybe a Bi-LSTM or Transformer. Ensemble the features described. If we have a large amount of labeled plagiarism file we can apply a supervised method. Otherwise we might need to apply a unsupervised method or create a labeled dataset ourselves. One idea on how to do that would be to use text and insert a random sentence from another text into it. Also the human expert still has to judge the findings of the model.
\item
	Data collection would need large amount of resources. Furthermore training the model on a large amount of data will take quite some time. Would definitely work well on the form of ``copy-paste plagiarism''. If somebody translates one language into another and uses this to plagiarize we would only detect it if the stylometry changes. ``Stealing'' ideas is also a form of plagiarism, which we could not detect that. I am not sure how that could even be possible to detect. We would also not check if the used citations are correct, so a writer might cite a wrong source or just make a source up in order to hide the plagiarism.
\end{enumerate}

\newpage \item \textit{Causality}. You are asked to build a system to extract causal statements from text of a manufacturing company with a lot of textual document, including technical reports. For example the sentence \newline\texttt{``Mechanical stress is one of the main causes of yield loss''} \newline should be automatically annotated as \newline\texttt{``\{Mechanical stress\}$_{Cause}$ is one of the main \{causes\}$_{Cue}$ of \{yield loss\}$_{Effect}$''}.
\begin{enumerate}
	\item What type of approach do you choose?
	\item What properties do you expect for your approach? E.g., better recall/precision, better performance on longer sentence?
\end{enumerate}

\begin{enumerate}
\item  
	We want to detect the span and roles of causality in the text. A machine learning approach seems to be the best fit. Probably some kind of deep learning like BERT (maybe we find a pretrained BERT-model that is trained on technical reports or similar) since those currently produce the best results (sequence classification). For training we need annotated corpora.
\item 
	Evaluation often struggles as there is often no ground truth. But we would expect similar precision and recall values, if trained successfully we would expect ``high'' values for these. The longer the sentence the harder it gets to find causal relations, since there could also be multiple (and convoluted) causal relations in a single long sentence. The causal relations could even span multiple sentences, I think for our approach detection in a single sentence should be sufficient.
\end{enumerate}


\newpage \item \textit{Evaluation}. You developed a method for style transfer for German text. Given a sentence written by an arbitrary writing style, you methods outputs the ``same'' text as it were written by a famous author (e.g., Thomas Mann, Wolf Haas).  Now you are requested to assess how well your system is working. 
\begin{enumerate}
	\item What evaluation methodology do you follow?
	\item What evaluation measures do you use?
	\item Are there known limitations in your evaluation methodology or evaluation measures?
\end{enumerate}

\begin{enumerate}
\item  
	Somehow create some ground truth that we can compare to, otherwise we need to involve humans that rate the output of the model. Might also make sense to use an explainable AI model, if possible. We also need a baseline to compare against (heuristic, human).
\item 
	Compare the stylometry of the output of the model with texts from the author we are trying to mimic. Ensemble some typical stylometric features like average word length, vocabulary richness measure, white space ratio, \dots\,. Maybe the individual authors have some consistent style that is very recognizable, look at what feature separates that author from the others. Another idea would be to use an already trained model that identifies authors (might be harder to find one that uses German text) and let that predict the author of the created texts by writing styles. We would probably get the ``most correct'' evaluation by using experts, but will be expensive.
\item 
	If we use humans to create a ground truth or to rate the output we also need to look at the inter-rater agreement. They might disagree in some aspects. If we use another model to rate the output of our prediction the total accuracy is also dependent on both models, could be hard to differentiate in terms of which part of the error corresponds to which model.
\end{enumerate}


\end{enumerate}\end{document}




